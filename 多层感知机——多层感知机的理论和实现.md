## 多层感知机

### 隐藏层

单层的线性回归或者分类都有可能面对边际递减效应的数据而缺乏应对，如下例子：我们需要衡量你开心的程度和你的工资差异，同样的线性增加8000r/月，你刚毕业时基础工资从4000涨到12000，比你到了中年，工资从80000涨到88000肯定来的开心的多，但是如果是线性模型，他会把这种差异直接的体现在了最后的对你的开心衡量上。

因此我们在网络中加入一个或多个隐藏层来克服线性模型的限制，从而能够处理更加普遍的函数类型关系，如下图所示：

![../_images/mlp.svg](C:\Users\裴英豪\Desktop\思维导图总结\第二个月\深度学习笔记\source\mlp.svg)

层之间的关系时全连接的，每个输入都会影响每个单元，所以我们能够很直觉的发现mlp的一个特征——计算开销很高。

> 非线性的引入：我们先来看一下正常的不增加非线性的mlp，能看出来经过多个参数计算后，最后的结果实际上还是一个单层的模型，这不是我们所想要的。
>
> $\mathbf{O} = (\mathbf{X} \mathbf{W}^{(1)} + \mathbf{b}^{(1)})\mathbf{W}^{(2)} + \mathbf{b}^{(2)} = \mathbf{X} \mathbf{W}^{(1)}\mathbf{W}^{(2)} + \mathbf{b}^{(1)} \mathbf{W}^{(2)} + \mathbf{b}^{(2)} = \mathbf{X} \mathbf{W} + \mathbf{b}.$
>
> 我们的做法是在每一层的输出后边加上一个激活函数（activation function）$\sigma$，激活函数一般是非线性的，通过激活函数的，我们的多层感知机不会退化成线性模型：
>
> $$\begin{split}\begin{aligned}
>     \mathbf{H} & = \sigma(\mathbf{X} \mathbf{W}^{(1)} + \mathbf{b}^{(1)}), \\
>     \mathbf{O} & = \mathbf{H}\mathbf{W}^{(2)} + \mathbf{b}^{(2)}.\\
> \end{aligned}\end{split}$$

 ### 激活函数

激活函数的价值我们在上文的引用中已经讨论过了，那么现在我们肯定知道激活函数是非线性的，我们可以看一下常用的激活函数，试着总结下他们的特征。

**ReLU函数**
$$
\operatorname{ReLU}(x) = \max(x, 0).
$$
我们在jupyter里画一下ReLU的图像：

```python
x = torch.arange(-8.0, 8.0, 0.1, requires_grad=True)
y = torch.relu(x)
d2l.plot(x.detach(), y.detach(), 'x', 'relu(x)', figsize=(5, 2.5))
```

![../_images/output_mlp_76f463_21_0.svg](C:\Users\裴英豪\Desktop\思维导图总结\第二个月\深度学习笔记\source\ReLU.svg)

然后我们可以看一下如何对ReLU进行求导:

```python
y.backward(torch.ones_like(x), retain_graph=True)#能发现这里的backward有个参数torch.ones_like(x)，我们下边会讨论他的作用
d2l.plot(x.detach(), x.grad, 'x', 'grad of relu', figsize=(5, 2.5))
```

![../_images/output_mlp_76f463_36_0.svg](C:\Users\裴英豪\Desktop\思维导图总结\第二个月\深度学习笔记\source\ReLU_grad.svg)

ReLU有一个变体pReLU，保证参数哪怕是个负数，有些信息也可以通过：
$$
\operatorname{pReLU}(x) = \max(0, x) + \alpha \min(0, x).
$$

#### 我们现在来关注下ReLU反向求导的那个代码中的参数。

在没有这个参数的时候，我们进行求导会出现：**RuntimeError: grad can be implicitly created only for scalar outputs**，即输出不是一个标量，这里我们从原理来说明：

pytorch实现反向求导是计算图机制，而且还是动态图，我们以$y=(a+b)(b+c)$的计算图为例：

![1684381860866](C:\Users\裴英豪\Desktop\思维导图总结\第二个月\深度学习笔记\source\计算图示例1.png)

叶节点是我们声明的实际数值，y是我们对声明的计算，我们之前获得的是一个标量y，相当于只占用了一个计算图，但是如果我们的求导是对一个向量$\hat y$进行求导，那么就需要多个不同的计算图进行计算了。retain_graph=True也是为了让计算图不被释放。

> 引用csdn上的结论就是：
>
> 如果是标量对向量求导(scalar对tensor求导)，那么就可以保证上面的计算图的根节点只有一个，此时不用引入grad_tensors参数，直接调用backward函数即可
> 如果是(向量)矩阵对(向量)矩阵求导(tensor对tensor求导)，实际上是先求出Jacobian矩阵中每一个元素的梯度值(每一个元素的梯度值的求解过程对应上面的计算图的求解方法)，然后将这个Jacobian矩阵与grad_tensors参数对应的矩阵进行对应的点乘，得到最终的结果。
>
> ------------------------------------------------
> 版权声明：本文为CSDN博主「Camlin_Z」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。
> 原文链接：https://blog.csdn.net/sinat_28731575/article/details/90342082

反向求导的简单理解到此结束，以后遇到了更复杂的情况，我会继续思考。

**sigmoid函数**
$$
\operatorname {sigmoid}(x) = \frac{1}{1+exp(-x)}
$$
sigmoid接受来自$\mathbb{R}$的输入，变换成区间（0，1）上的输出。

同样我们画出它的图像：

```python
y = torch.sigmoid(x)
d2l.plot(x.detach(), y.detach(), 'x', 'sigmoid(x)', figsize=(5, 2.5))
```



![../_images/output_mlp_76f463_51_0.svg](C:\Users\裴英豪\Desktop\思维导图总结\第二个月\深度学习笔记\source\sigmoid.svg)

我们可以计算它的导数：
$$
\frac{d}{dx}\operatorname{sigmoid}(x)=\frac{exp(-x)}{(1+exp(-x))^2}=\operatorname{sigmoid(x)}\left(1-\operatorname{sigmoid}(x)\right).
$$
并画它的图像：

```python
# 清除以前的梯度
x.grad.data.zero_()
y.backward(torch.ones_like(x),retain_graph=True)
d2l.plot(x.detach(), x.grad, 'x', 'grad of sigmoid', figsize=(5, 2.5))
```

![../_images/output_mlp_76f463_66_0.svg](C:\Users\裴英豪\Desktop\思维导图总结\第二个月\深度学习笔记\source\sigmoid_grad.svg)

**tanh函数**

同上边一样，我们不难实现，其定义为:
$$
\operatorname{tanh}(x) = \frac{1 - \exp(-2x)}{1 + \exp(-2x)}.
$$
画出图像:

```python
y = torch.tanh(x)
d2l.plot(x.detach(), y.detach(), 'x', 'tanh(x)', figsize=(5, 2.5))
```

![../_images/output_mlp_76f463_81_0.svg](C:\Users\裴英豪\Desktop\思维导图总结\第二个月\深度学习笔记\source\tanh.svg)

画出导数：

```python
# 清除以前的梯度
x.grad.data.zero_()
y.backward(torch.ones_like(x),retain_graph=True)
d2l.plot(x.detach(), x.grad, 'x', 'grad of tanh', figsize=(5, 2.5))
```

![../_images/output_mlp_76f463_96_0.svg](C:\Users\裴英豪\Desktop\思维导图总结\第二个月\深度学习笔记\source\tanh——grad.svg)





## 多层感知机的从零实现

#### 确定批量大小和读取数据集

```python
import torch
from torch import nn
from d2l import torch as d2l

batch_size = 256
train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)#这个函数在上一章被定义了，返回一个包含了batchsize大小的迭代器
```

#### 初始化模型的参数

我们首先来思考一下，第一层的输入直接来自于图片，也就是784，那么他需要多少输出呢？这取决于第二层的隐藏层的隐藏单元的个数，这里是256，故而第一层的参数w的size就应该是（786，256），第二层的输入是256，输出类别的个数10，因此$w_2$的size应该是（256，10），我们用num_inputs记录外界的输入，num_outputs记录输出的类别个数，num_hiddens记录隐藏的神经元个数256.

```python
num_inputs, num_outputs, num_hiddens = 784, 10, 256

W1 = nn.Parameter(torch.randn(
    num_inputs, num_hiddens, requires_grad=True) * 0.01)
b1 = nn.Parameter(torch.zeros(num_hiddens, requires_grad=True))
W2 = nn.Parameter(torch.randn(
    num_hiddens, num_outputs, requires_grad=True) * 0.01)
b2 = nn.Parameter(torch.zeros(num_outputs, requires_grad=True))

params = [W1, b1, W2, b2]
```

#### 激活函数

```python
def relu(X):
    a=torch.zeros_like（X）
    return torch.max(X,a)
```

#### 模型与损失函数

我们要把两层的模型构建到一个函数里，并且把矩阵24*24的输入转换成一个向量。

```python
def net(X):
    X=X.reshape((-1,num_inputs))
    H=relu(X@W1+b1)#这里的‘@’代表矩阵乘法
    return（H@w2+b2）
loss = nn.CrossEntropyloss(reduction='none')
```

#### 训练

我们先调用包，然后再回顾一下我们的训练。

```python
num_epochs,lr = 10,0.1
updater = torch.optim.SGD(params,lr=lr)#参数更新的方法采用随机梯度下降
d2l.train_ch3(net,train_iter,test_iter,loss,num_epochs,updater)
```

这是我们上一节定义的train_ch3,我们来阅读下：

```python
def train_ch3(net, train_iter, test_iter, loss, num_epochs, updater):  #@save
    """训练模型（定义见第3章）"""
    animator = Animator(xlabel='epoch', xlim=[1, num_epochs], ylim=[0.3, 0.9],
                        legend=['train loss', 'train acc', 'test acc'])#画动画的
    for epoch in range(num_epochs):#控制我们训练的轮数
        train_metrics = train_epoch_ch3(net, train_iter, loss, updater)#只进行一轮的训练，获得训练损失和训练精度
        test_acc = evaluate_accuracy(net, test_iter)#评估在测试机上的精确度
        animator.add(epoch + 1, train_metrics + (test_acc,))#增加数据点
    train_loss, train_acc = train_metrics#获得训练集的损失和精确度
    assert train_loss < 0.5, train_loss
    assert train_acc <= 1 and train_acc > 0.7, train_acc
    assert test_acc <= 1 and test_acc > 0.7, test_acc
```

我们再看一下，实际上这个函数的重点是在与每一轮的训练和参数更新，我们把每一轮的参数更新函数‘train_epoch_ch3’拿出来看看：

```python
def train_epoch_ch3(net, train_iter, loss, updater):  #@save
    """训练模型一个迭代周期（定义见第3章）"""
    # 将模型设置为训练模式
    if isinstance(net, torch.nn.Module):#isinstance是用来判断有一个对象是否一个已知的对象
        net.train()
    # 记录训练损失总和、训练准确度总和、样本数
    metric = Accumulator(3)
    for X, y in train_iter:
        # 计算梯度并更新参数
        y_hat = net(X)#获得预测
        l = loss(y_hat, y)
        if isinstance(updater, torch.optim.Optimizer):
            # 如果updater已经存在在Torch里了，就使用PyTorch内置的优化器和损失函数
            updater.zero_grad()#梯度清零
            l.mean().backward()#损失函数反向求导
            updater.step()#updater是一个可定义的更新模型参数w的函数
        else:
            # 使用定制的优化器和损失函数
            l.sum().backward()
            updater(X.shape[0])
        metric.add(float(l.sum()), accuracy(y_hat, y), y.numel())
    # 返回训练损失和训练精度
    return metric[0] / metric[2], metric[1] / metric[2]
```

> 我们生活的每个日常，都是连续发生的奇迹。
>
> <p align="right">——《日常》</p>



## MLP的简洁实现

### 存储网络

```python
net = nn.Sequential(nn.Flatten(),
                    nn.Linear(784, 256),
                    nn.ReLU(),
                    nn.Linear(256, 10))#我们用一个sequential容器存储我们每一层的网络，注意我们这里在网络的第一层加入了一个展平层，而不是手动操作。

def init_weights(m):
    if type(m) == nn.Linear:
        nn.init.normal_(m.weight, std=0.01)#我们使用的参数初始化方法为正态随机，但是He初始化可能会更好一些。

net.apply(init_weights);#应用括号里的函数去初始化每个其中的参数
```

### 模型训练

```python
batch_size, lr, num_epochs = 256, 0.1, 10
loss = nn.CrossEntropyLoss(reduction='none')
trainer = torch.optim.SGD(net.parameters(), lr=lr)

train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)
d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)
```


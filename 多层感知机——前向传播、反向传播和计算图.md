## 前向传播、反向传播和计算图



这一部分的内容有一个很好的讲解视频，[如何理解“梯度下降法”？什么是“反向传播”？通过一个视频，一步一步全部搞明白]( https://www.bilibili.com/video/BV1Zg411T71b/?share_source=copy_web&vd_source=b9fad40156ac06edc803202851dbf50c),这里我把我的观后感和理解进行总结表述。

### 前向传播（计算每个神经元的输出值）

> *前向传播*（forward propagation或forward pass） 指的是：按顺序（从输入层到输出层）计算和存储神经网络中每层的结果。

首先我们明确，前向传播可以进行梯度计算，但是由于他的缺点（比较耗内存，容易出错），我们一般使用反向传播计算误差，只使用前向传播把数值给进行计算。

我们以全连接为例：![1684820335796](C:\Users\裴英豪\Desktop\思维导图总结\第二个月\深度学习笔记\source\mlp6.png)

他有多个隐藏层，且每个隐藏层有一个激活函数，所以我们可以用下边的这个公式来表达一个隐藏层的输入输出，我们先来对每个符号进行一下解释：$f_l( ·)$表示第l层的激活函数，$W^{(l)}$表示第l-1层到第l层的权重矩阵，$b^{(l)}$表示第l-1层到第l层的偏置，$z^{(l)}$表示第l层神经元的净输入（净活性质），$a^{(l)}$表示第l层神经元（第一层的输入是$x=a^{(0)}$）的输出（活性值）。

首先我们关注第l层的神经元获得的输入，来自第l-1层的$a^{l-1}$,则我们可以获得的净活性值为$z^{(l)}=W^{(l)} a^{(l-1)} + b^{(l)}$,然后送入激活函数得到第l层神经元的活性值:$a^{(l)}=f_l(z^{(l)})$。

上述内容可以简写为：
$$
\boldsymbol{z}^{(l)}=\boldsymbol{W}^{(l)} \cdot f_{l-1}\left(\boldsymbol{z}^{(l-1)}\right)+\boldsymbol{b}^{(l)}\\或者\\
\boldsymbol{a}^{(l)}=f_l\left(\boldsymbol{W}^{(l)} \cdot \boldsymbol{a}^{(l-1)}+\boldsymbol{b}^{(l)}\right)
$$
如果我们只关注数据的流向，结果就是:
$$
\left.\boldsymbol{x}=\boldsymbol{a}^{(0)} \rightarrow \boldsymbol{z}^{(1)} \rightarrow \boldsymbol{a}^{(1)} \rightarrow \boldsymbol{z}^{(2)} \rightarrow \cdots \rightarrow \boldsymbol{a}^{(L-1)} \rightarrow \boldsymbol{z}^{(L)} \rightarrow \boldsymbol{a}^{(L)}=\phi(\boldsymbol{x} ; \boldsymbol{W}, \boldsymbol{b})\right)
$$


**前向传播计算图**可以进一步帮助我们理解操作符和变量的依赖关系，如:

![../_images/forward.svg](C:\Users\裴英豪\Desktop\思维导图总结\第二个月\深度学习笔记\source\forward.svg)

而如果我们就此进行损失计算，那么单个样本的损失项为:$L = l(\mathbf{o}, y).$,给定$L_2$正则化的系数$\lambda$,正则化项为:$s = \frac{\lambda}{2} \left(\|\mathbf{W}^{(1)}\|_F^2 + \|\mathbf{W}^{(2)}\|_F^2\right)$.最后就可以算出在给定样本上的正则化损失为:
$$
J = L + s.
$$
J就是我们的目标函数（objective function）



### 反向传播

我们最后的目的是获得计算神经网络参数的梯度。简而言之，反向传播就是利用微积分中的链式法则，从**输出层到输入层**遍历网络。并使用计算图存储了计算某些参数需要的任何中间变量（偏导数）。如$\mathsf{Y}=f(\mathsf{X})$和$\mathsf{Z}=g(\mathsf{Y})$,其中输入和输出XYZ是任意形状的张量，则Z关于X的导数是:$\frac{\partial \mathsf{Z}}{\partial \mathsf{X}} = \text{prod}\left(\frac{\partial \mathsf{Z}}{\partial \mathsf{Y}}, \frac{\partial \mathsf{Y}}{\partial \mathsf{X}}\right).$

prod代表了相乘（矩阵求导中的链式法则要求从右往左乘）。

接下来我们具体分析，我们要求损失函数对某第l层的第i行j列的参数$w_{ij}^{(l)}$进行求导，我们可以利用链式法则:
$$
\frac{\delta L(y,\hat y)}{\delta w_{ij}^{(l)}}=\frac{\delta z^{(l)}}{\delta w_{ij}^{(l)}}\frac{\delta L(y,\hat y)}{\delta z^{(l)}}\\
\frac{\delta L(y,\hat y)}{\delta b^{(l)}}=\frac{\delta z^{(l)}}{\delta b^{(l)}}\frac{\delta L(y,\hat y)}{\delta z^{(l)}}\\
$$
从上式可以看出来，我们的梯度主要由三个基础的偏导数$\frac{\delta z^{(l)}}{\delta w_{ij}^{(l)}},\frac{\delta z^{(l)}}{\delta b^{(l)}}$，和一个最关键的$,\frac{\delta L(y,\hat y)}{\delta z^{(l)}},\\$构成，前两个都是单独的一层之间的偏导数，后边的偏导数直接与损失函数相关。

我们先来计算偏导数$\frac{\delta z^{(l)}}{\delta w_{ij}^{(l)}}$,因为$z^{(l)}=W^{(l)} a^{(l-1)} + b^{(l)}$,而$w_{ij}$是一个标量且只与对应位置的$a_{ij}$作乘法，所以有：最后的结果显示了这一个偏导数的大小只与他的输入有关，而和其它无关。

![1684830772797](C:\Users\裴英豪\Desktop\思维导图总结\第二个月\深度学习笔记\source\z-ij.png)

然后我们计算偏导数$\frac{\delta z^{(l)}}{\delta b^{(l)}}$，从我们的式子里$z^{(l)}=W^{(l)} a^{(l-1)} + b^{(l)}$可以看出，z和b都是相同维度的向量，且z是单纯的b的和，那么$\frac{\delta z^{(l)}}{\delta b^{(l)}}$的结果就是一个单位矩阵。

最后是最重要的一部分$\frac{\delta L(y,\hat y)}{\delta z^{(l)}}$，它表示第l层神经元对最终损失的影响，也反映了**最终损失对第l层神经元的敏感程度**，因此一般称为第l层神经元的误差项用$\delta^{(l)}$来表示：
$$
\delta^{(l)}=\frac{\delta L(y,\hat y)}{\delta z^{(l)}} \in ℝ^{𝑀_𝑙}
$$
根据 $\boldsymbol{z}^{(l+1)}=\boldsymbol{W}^{(l+1)} \boldsymbol{a}^{(l)}+\boldsymbol{b}^{(l+1)}$, 有
$$
\frac{\partial \boldsymbol{z}^{(l+1)}}{\partial \boldsymbol{a}^{(l)}}=\left(\boldsymbol{W}^{(l+1)}\right)^{\top} \quad \in \mathbb{R}^{M_l \times M_{l+1}} .
$$
根据 $\boldsymbol{a}^{(l)}=f_l\left(\boldsymbol{z}^{(l)}\right)$, 其中 $f_l(\cdot)$ 为按位计算的函数, 因此有
$$
\begin{aligned}
\frac{\partial \boldsymbol{a}^{(l)}}{\partial \boldsymbol{z}^{(l)}} & =\frac{\partial f_l\left(\boldsymbol{z}^{(l)}\right)}{\partial \boldsymbol{z}^{(l)}} \\
& =\operatorname{diag}\left(f_l^{\prime}\left(\boldsymbol{z}^{(l)}\right)\right) \quad \in \mathbb{R}^{M_l \times M_l} .
\end{aligned}
$$
因此, 根据链式法则, 第 $l$ 层的误差项为
$$
\delta^{(l)} \triangleq \frac{\partial \mathcal{L}(\boldsymbol{y}, \hat{\boldsymbol{y}})}{\partial \boldsymbol{z}^{(l)}}
$$
$$
\begin{aligned}
& =\frac{\partial \boldsymbol{a}^{(l)}}{\partial \boldsymbol{z}^{(l)}} \cdot \frac{\partial \boldsymbol{z}^{(l+1)}}{\partial \boldsymbol{a}^{(l)}} \cdot \frac{\partial \mathcal{L}(\boldsymbol{y}, \hat{\boldsymbol{y}})}{\partial \boldsymbol{z}^{(l+1)}} \\
& =\operatorname{diag}\left(f_l^{\prime}\left(\boldsymbol{z}^{(l)}\right)\right) \cdot\left(\boldsymbol{W}^{(l+1)}\right)^{\top} \cdot \delta^{(l+1)} \\
& =f_l^{\prime}\left(\boldsymbol{z}^{(l)}\right) \odot\left(\left(\boldsymbol{W}^{(l+1)}\right)^{\top} \delta^{(l+1)}\right) \in \mathbb{R}^{M_l},
\end{aligned}
$$

你最后可以发现第l层的误差项变成了第l+1层的误差项和l+1层的参数矩阵以及激活函数对第l层的净激活值的偏导的乘积，就可以轻易地通过前向传播的计算结果和反向传播的误差，算出每一层的误差项。

获得了误差项，我们的偏置就容易得到了L(𝒚, 𝒚)̂ 关于第𝑙 层权重𝑾(𝑙) 和第l层偏置置𝒃(𝑙)的梯度为：
$$
\frac{𝜕L(y,\hat y)}{𝜕W^{(𝑙)}} = 𝛿^{(𝑙)}(𝒂(𝑙−1))\\
\frac{𝜕L(y,\hat y)̂}{𝜕b^{(𝑙)}} = 𝛿^{(𝑙)}
$$

#### 总结

在计算每一层的误差项之后，我们就可以得到每一层参数的梯度，所以从理论的角度来说，使用误差项反向传播的前馈神经网络训练过程可以分为以下三步：

 前馈计算每一层的净输入𝒛(𝑙) 和激活值𝒂(𝑙)，直到最后一层；

 反向传播计算每一层的误差项𝛿(𝑙)；

 计算每一层参数的偏导数，并更新参数
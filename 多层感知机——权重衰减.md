# 权重衰减

## 

> 正则化：可以减少测试误差而非训练误差的方法被称为正则化。正则化是除了获得更多的数据之外的一种用来解决过拟合的常用方。

在正常的多项式回归情况下，带有阶数d的项数迅速增加，如，给定k个变量，阶数为d的项的个数为${k - 1 + d} \choose {k - 1}$

即$C^{k-1}_{k-1+d} = \frac{(k-1+d)!}{(d)!(k-1)!}$。因此即使是阶数上的细小变化，也会显著影响模型复杂度。但单纯的限制只能有多少阶太过粗暴，我们需要细粒度的限制——**范数和权重衰减**

#### 范数与L2正则化（norm and L2 regularization）

L2正则化通过改变目标函数（损失函数），增加惩罚项（penalty），进而改变参数求导的方向，促使其减少模型复杂度。

在传统的损失函数里（以线性回归为例），损失由$L(\mathbf{w}, b) = \frac{1}{n}\sum_{i=1}^n \frac{1}{2}\left(\mathbf{w}^\top \mathbf{x}^{(i)} + b - y^{(i)}\right)^2.$给出，$\mathbf x^{(i)}$是样本的特征，$y^{(i)}$是样本i的标签，$(\mathbf{w},b)$是权重和偏置参数，为了惩罚那些权重向量过大，我们要在损失函数中添加$\| \mathbf{w}| |^2$,由此我们的损失变成了:$L(\mathbf{w}, b) + \frac{\lambda}{2} \|\mathbf{w}\|^2$.

如果你曾了解过岭回归（ridge regression），这个样子可以说是一般无二。通过给损失函数加上了范数的枷锁，我们可以有效地保证算法在大量特征上均匀的分布权重，从而一定程度保证了模型的稳定。



#### 权重衰减（weight decay）

顾名思义，权重衰减就是在层与层的权重参数传递时，以一定的衰减系数，衰减系数越大对参数$\mathbf w$的约束越大。如下图，这就是我们在 $\mathbf{ L_2}$正则化回归的小批量随机梯度下降更新如下式：

$$
\begin{aligned}
\mathbf{w} & \leftarrow \left(1- \eta\lambda \right) \mathbf{w} - \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \mathbf{x}^{(i)} \left(\mathbf{w}^\top \mathbf{x}^{(i)} + b - y^{(i)}\right).
\end{aligned}
$$


### 权重衰减演示

导入包

```python
%matplotlib inline
import torch
from torch import nn
from d2l import torch as d2l
```

接下来我们生成一些数据，我们想要生成的数据是：
$$ {2}
y = 0.05 + \sum_{i = 1}^d 0.01 x_i + \epsilon \text{ where }
\epsilon \sim \mathcal{N}(0, 0.01^2).
$$ {2}

```python
n_train, n_test, num_inputs, batch_size = 20, 100, 200, 5#训练集20，测试集100，标签为200，批量大小为5
true_w, true_b = torch.ones((num_inputs, 1)) * 0.01, 0.05#偏置全1，
train_data = d2l.synthetic_data(true_w, true_b, n_train)#这是一个生成随机数据集的函数
train_iter = d2l.load_array(train_data, batch_size)#获得训练迭代器
test_data = d2l.synthetic_data(true_w, true_b, n_test)
test_iter = d2l.load_array(test_data, batch_size, is_train=False)
```

#### 从零开始

初始化模型

```python
def init_params():
    w = torch.normal(0, 1, size=(num_inputs, 1), requires_grad=True)
    b = torch.zeros(1, requires_grad=True)
    return [w, b]
```

定义$\mathbb L_2$范数乘法：

```python
def l2_penalty(w):
    return torch.sum(w.pow(2)) / 2
```

定义训练代码实现：

```python
def train(lambd):
    w,b=init_params()
    # 这里对net赋值lambda X: d2l.linreg(X, w, b)，
    # 是用到python匿名函数lambda [parameters]: expression。即调用net(X)就相当于调用d2l.linreg(X, w, b)
    # 匿名函数不需要return获取返回值，表达式本身结果就是返回值。
    # 匿名函数优点就是不需要起函数名
    net,loss = lambda X:d2l.linreg(X, w, b),d2l.squared_loss
    num_epochs,lr=100,0.003
    animator = d2l.Animator(xlabel='epochs', ylabel='loss', yscale='log',
                            xlim=[5, num_epochs], legend=['train', 'test'])
    for epoch in range(num_epochs):# 对每一次数据迭代
        for X ,y in train_iter:# 每一次从数据迭代器里拿出X,y
            #增加L2范数惩罚项
            #广播机制使得L2_penalty(w)成为一个长度为batch_size大小的向量
            l=loss(net(X),y)+lambd*l2_penalty(w)
            l.sum().backward()
            d2l.sgd([w,b],lr,batch_size)
        if(epoch+1)%5==0:
            animator.add(epoch + 1, (d2l.evaluate_loss(net, train_iter, loss),
                                     d2l.evaluate_loss(net, test_iter, loss)))
    print('w的L2范数是：', torch.norm(w).item())
```

接下来我们展示lambd不同值的情况：

首先是lambd=0，禁用权重衰减：

```python
train(lambd=0)
>>w的L2范数是： 13.920683860778809
```

![1684557665946](C:\Users\裴英豪\Desktop\思维导图总结\第二个月\深度学习笔记\source\lambd=0.png)

```python
train(lambd=3)
>>w的L2范数是： 0.379617840051651
```

![1684557708655](C:\Users\裴英豪\Desktop\思维导图总结\第二个月\深度学习笔记\source\lambd=3.png)

我们来看看真实的w的范数是多少：

```python
true_w.norm().item()
>>0.14142125844955444
```

虽然仍有差距，但是明显加了正则化的模型对抗过拟合的效果更好。

这里补充一个调用Torch接口的代码;

```python
def train_concise(wd):
    '''wd是权重衰减的lambd'''
    net = nn.Sequential(nn.Linear(num_inputs,1))#构建一个输出为1，输入为num_inputs的线性层
    for param in net.parameters():#获取每一层的参数
        param.data.normal_()#给所有参数都赋值为正态
    loss = nn.MSELoss(reduction='none')
    num_epochs,lr=100,0.003
    #偏置参数没有衰减
    trainer = torch.optim.SGD([
        {'params':net[0].weight,'weight_decay':wd},
        {'params':net[0].bias}],lr=lr)
    animator = d2l.Animator(xlabel='epochs', ylabel='loss', yscale='log',
                            xlim=[5, num_epochs], legend=['train', 'test'])
    for epoch in range(num_epochs):
        for X,y in train_iter:
            trainer.zero_grad()
            l=loss(net(X),y)
            l.mean().backward()
            trainer.step()
        if (epoch+1)%5==0:
            animator.add(epoch + 1,
                         (d2l.evaluate_loss(net, train_iter, loss),
                          d2l.evaluate_loss(net, test_iter, loss)))
    print('w的L2范数：', net[0].weight.norm().item())
```

其它跟从零实现的一样。







### 补充L1-penalt

首先是不限制的：

![1684571011314](C:\Users\裴英豪\Desktop\思维导图总结\第二个月\深度学习笔记\source\L1-lambd=0.png)

其次是限制为lambd=3的：

![1684571070449](C:\Users\裴英豪\Desktop\思维导图总结\第二个月\深度学习笔记\source\L1-lambd=3.png)

非常清楚地可以看出来，L1penalty的最后出现了比较明显的震荡，可能在于L1最后的判断比较偏向个例，容易被特殊个例影响。
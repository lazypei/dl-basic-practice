## 环境与分布偏移

> 一般而言，样本的原始特征中的每一维特征由于来源以及度量单位不同，其特征取值的分布范围往往差异很大. 当我们计算不同样本之间的欧氏距离时，取值范围大的特征会起到主导作用. 这样，对于基于相似度比较的机器学习方法，必须对样本进行预处理**将各个维度的特征归一化到同一个取值区间，并且消除不同特征之间的相关性，才能获得比较理想的结果.**

我们假设一个贷款申请人违约风险模型，用来预测谁将违约或者偿还贷款。这个模型会发现申请人的鞋子和是否违约有相关性（穿牛津鞋申请人会偿还，穿运动鞋申请人会违约）。此后哪怕有个信誉很好的富豪穿了运动鞋，模型也会认为他违约。

### 首先我们考虑分布的类型

#### 协变量偏移

>在传统机器学习中， 一个常见的问题是协变量偏移（Covariate Shift）. 协变量是一个统计学概念，是可能影响预测结果的统计变量.在机器学习中，协变量可以看作是输入. 一般的机器学习算法都要求输入在训练集和测试集上的分布是相似的. 如果不满足这个假设，在训练集上学习到的模型在测试集上的表现会比较差.
>
>![![1684917781835](C:\Users\裴英豪\Desktop\思维导图总结\第二个月\深度学习笔记\source\Covariate.png)1684917781835](C:\Users\裴英豪\AppData\Roaming\Typora\typora-user-images\1684917781835.png)

我们来用一个直观的例子，区分两种动物：猫和狗。下边的照片是我们的训练集

![../_images/cat-dog-train.svg](C:\Users\裴英豪\Desktop\思维导图总结\第二个月\深度学习笔记\source\cat-dog-train.svg)

当我们在训练集上不断迭代，但在测试的时候，我们面对的数据是下面的:

![../_images/cat-dog-test.svg](C:\Users\裴英豪\Desktop\思维导图总结\第二个月\深度学习笔记\source\cat-dog-test.svg)

我们的模型如果不适应这种变化的特征输入，那么训练了等于白训练。

#### 标签偏移

标签偏移是指训练集和测试集之间的目标变量的概率分布存在差异。如果在医疗诊断领域，我们可能有两个互相关联的变量——症状和疾病，其中症状往往比疾病更容易获得。如果我们只基于症状来训练模型来预测疾病，那么在测试集中，疾病的相对频率分布可能会发生变化，从而导致模型表现下降。

我的理解就是，标签的频率发生变化了，而我们的模型还按照训练集上的频率去进行分配。

#### 概念偏移

一天我去吃饭，我要吃扁食，我对象说好啊，然后我买了两盘饺子，我对象很奇怪，扁食不是馄饨吗。

我认为的扁食是饺子，同一种东西的不同名称，不同东西的同一种名称，给我们训练带来了压力。

### 分布偏移纠正

多数时候，我们得到的数据不会像训练集上那么标准，下面我们讨论面对那些不好的偏移，我们怎么做才能减少这种偏移。

#### 经验风险与实际风险

在我们的训练集上，我们的目标是为了减少经验风险，也就是:
$$
\mathop{\mathrm{minimize}}_f \frac{1}{n} \sum_{i=1}^n l(f(\mathbf{x}_i), y_i),
$$
其中l就是我们前边经常用到的损失函数，用来度量我们的预测函数$f(x_i)$与训练集上的标签$y_i$的差距，经验风险是我们为了模拟真实情况的风险。

> 实际的情况下我们需要面临过拟合，所以容易产生我们前边提到的过拟合，实际上我们的训练也不会是追求经验风险最小化，而是追求结构风险最小化:
> $$
> \mathop{\mathrm{minimize}}_f \frac{1}{n} \sum_{i=1}^n l(f(\mathbf{x}_i), y_i)+\frac{1}{2}\lambda||\theta||^2
> $$
> 其中$||\theta||^2$是$l_2$范数的正则化项，用来避免过拟合，$\lambda$是用来控制正则化强度的。

#### 协变量偏移纠正

比如我们的训练模型里有十条狗，两只猫，我的测试模型里有四只猫一只狗，那么对狗的判断条件一定会过于宽泛（想想svm），那么我们直觉的就需要一个正则项来避免对狗判断的过于宽泛，源分布是$q(\mathbf{x})$，我们的测试集的分布$p(\mathbf{x})$.这里我们需要有个前提，即条件分布不变:$p(y \mid \mathbf{x}) = q(y \mid \mathbf{x})$，下边就可以通过 一个等式进行纠正:
$$
\begin{aligned}
\int\int l(f(\mathbf{x}), y) p(y \mid \mathbf{x})p(\mathbf{x}) \;d\mathbf{x}dy =
\int\int l(f(\mathbf{x}), y) q(y \mid \mathbf{x})q(\mathbf{x})\frac{p(\mathbf{x})}{q(\mathbf{x})} \;d\mathbf{x}dy.
\end{aligned}
$$
换句话说，我们需要通过控制来自不同分布的频率不同，来衡量样本权重的不同:
$$
\beta_i \stackrel{\mathrm{def}}{=} \frac{p(\mathbf{x}_i)}{q(\mathbf{x}_i)}.
$$
将权重带入到每个数据样本$(\mathbf{x}_i, y_i)$，里我们的损失函数最小化的模型变成了:
$$
\mathop{\mathrm{minimize}}_f \frac{1}{n} \sum_{i=1}^n \beta_i l(f(\mathbf{x}_i), y_i).
$$
我们转头来看看猫狗的例子，我们的训练集上狗的分布是$\frac 5 6$,在测试集上是$\frac 15$那么最后的样本权重就是：
$$
\beta_i {=} \frac{p(\mathbf{x}_i)}{q(\mathbf{x}_i)}=\frac {\frac15}{\frac56}=\frac{6}{25}.
$$
很容易理解，如果我们对狗的要求过高，那么他就会趋向于过拟合，但是如果我们降低它的训练要求（损失函数房小），他就能在一定的程度上避免过拟合。

而在实际的过程中，我们往往不能那么容易知道频率的分布，我们的选择就是进行抽样，抽取相同个数的样本。用z标签表示，从p中抽取的数据为1，从q中抽取的数据Wie-1.混合数据集中的概率由下式给出：
$$
P(z=1 \mid \mathbf{x}) = \frac{p(\mathbf{x})}{p(\mathbf{x})+q(\mathbf{x})} \text{ and hence } \frac{P(z=1 \mid \mathbf{x})}{P(z=-1 \mid \mathbf{x})} = \frac{p(\mathbf{x})}{q(\mathbf{x})}.
$$
因此，若我们使用对数几率回归方法，其中$P(z=1 \mid \mathbf{x})=\frac{1}{1+\exp(-h(\mathbf{x}))}$（h是一个参数化函数），则自然有：
$$
\beta_i = \frac{1/(1 + \exp(-h(\mathbf{x}_i)))}{\exp(-h(\mathbf{x}_i))/(1 + \exp(-h(\mathbf{x}_i)))} = \exp(h(\mathbf{x}_i)).
$$
**总之，我们的纠正算法是：假设一个训练集$\{(\mathbf{x}_1, y_1), \ldots, (\mathbf{x}_n, y_n)\}$和一个未标记的测试集$\{\mathbf{u}_1, \ldots, \mathbf{u}_m\}$。对于协变量偏移，我们假设$1 \leq i \leq n$的$x_i$来自某个源分布，$\mathbf{u}_i$来自目标分布。以下便是经典算法:

生成一个二元分类训练集:$\{(\mathbf{x}_1, -1), \ldots, (\mathbf{x}_n, -1), (\mathbf{u}_1, 1), \ldots, (\mathbf{u}_m, 1)\}$。

用**对数几率回归训练二元分类器**获得函数$h$。

使用$\beta_i = \exp(h(\mathbf{x}_i))$或者更好的$\beta_i = \min(\exp(h(\mathbf{x}_i)), c)$（c为常亮）对训练数据进行加权。

使用权重$\beta_i$进行$\mathop{\mathrm{minimize}}_f \frac{1}{n} \sum_{i=1}^n \beta_i l(f(\mathbf{x}_i), y_i).$的训练。



#### 标签偏移纠正

其实上边那个对于分狗和猫的例子更适合表达标签的纠正，所以基本选择同样的方法，甚至更为简单，我们可以直接选择那种频率的方法（我认为的。）



#### 概念偏移纠正

这个修正不存在很原则性的方法，我们可以更改网络权重以适应概念的出现，而不是从头开始训练（这会省去很多时间）





### 学习问题的分类法

（这部分内容应该在统计学习方法会有更深的理解，李沐的这一部分写的不算很好😅）
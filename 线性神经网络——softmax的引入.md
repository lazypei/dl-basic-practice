# softmax

## 如何实现分类问题的表达？

> 从一个例子出发：我们从一个图像分类问题开始。 假设每次输入是一个2×2的灰度图像。 我们可以用一个标量表示每个像素值，每个图像对应四个特征x1,x2,x3,x4。 此外，假设每个图像属于类别“猫”“鸡”和“狗”中的一个。
>
> 接下来，我们要选择如何表示标签。 我们有两个明显的选择：最直接的想法是选择y∈{1,2,3}， 其中整数分别代表{狗,猫,鸡}。 这是在计算机上存储此类信息的有效方法。 如果类别间有一些自然顺序， 比如说我们试图预测{婴儿,儿童,青少年,青年人,中年人,老年人}， 那么将这个问题转变为回归问题，并且保留这种格式是有意义的。
>
> 而想要表达这种不同的类别，一种方法是使用one-hot encoding（独热编码）,其中(1,0,0)对应于“猫”、(0,1,0)对应于“鸡”、(0,0,1)对应于“狗”：

$$
y\in \{(1,0,0),(0,1,0),(0,0,1)\}
$$

用自己的话来讲就是将多个输入输出为有限的特征。

## 网络的架构（softmax）：

> ![../_images/softmaxreg.svg](C:\Users\裴英豪\Desktop\思维导图总结\第二个月\深度学习笔记\source\softmaxreg.svg)

$$
o_1 = x_1w_{11}+x_2w_{12}+x_3w_{13}+x_4w_{14}+b_1\\
o_2 = x_1w_{21}+x_2w_{22}+x_3w_{23}+x_4w_{24}+b_2\\
o_3 = x_1w_{31}+x_2w_{32}+x_3w_{33}+x_4w_{34}+b_3\\
$$

如果使用线性代数符号，那么就是$o=Wx+b$

而为了满足概率的非负性以及所有样本概率之和为1，我们使用softmax对未规范的o进行规范，如下图所示。
$$
\hat{\mathbf{y}} = \mathrm{softmax}(\mathbf{o})\quad \text{其中}\quad \hat{y}_j = \frac{\exp(o_j)}{\sum_k \exp(o_k)}
$$
我们举一个简单的例子，假设输出的$o_1=0.3,o_2=0.7,0_3=1$,这显然不和我们希望得到的离散概率分布形式相符合，那么我们利用softmax，使:
$$
y_1=\frac{e^0.3}{e^0.3+e^0.7+e^1}\\
y_2=\frac{e^0.7}{e^0.3+e^0.7+e^1}\\
y_3=\frac{e^1}{e^0.3+e^0.7+e^1}\\
$$
通过这种方法，我么输出的$\vec{y}$就是一个满足要求的概率分布了。



## 损失函数

### 对数似然

softmax输出一个$\hat y$,我们可以将其视为‘每一类的条件概率’，如$P(y=\text{猫} \mid \mathbf{x})$。
$$
P(\mathbf{Y} \mid \mathbf{X}) = \prod_{i=1}^n P(\mathbf{y}^{(i)} \mid \mathbf{x}^{(i)}).
$$
(注意：后边的符号是连乘，也就是我们在数学一求参数时候经常写的多个分布相乘，而这背后隐藏的思想就是寻找一个参数，使现存的情况在模型中出现的概率最大)

然后对两边取对数,对每个样本的损失函数进行求和，$l(\mathbf{y}, \hat{\mathbf{y}}) = - \sum_{j=1}^q y_j \log \hat{y}_j$,不要被求和吓到，我们来举一个简单的例子，如果一张图片是猫，也就是$y=(0,1,0)$,由于我们通过softmax只能获得一个大概的值如${0.2,0.7,0.1}$，所以对应这张图片的单个损失函数$l(\mathbf{y}, \hat{\mathbf{y}}) =-(0\times \log(0.2)+1\times\log(0.6)+0\times\log(0.1))=-log(0.6)$,但是如果有一个模型预测到了$(0,1,0)$也就是完全符合真实情况，那么他的损失函数$l(\mathbf{y}, \hat{\mathbf{y}}) =-(0\times \log(0)+1\times\log(1)+0\times\log(0))=0$这就是他的一个案例表示
$$
-\log P(\mathbf{Y} \mid \mathbf{X}) = \sum_{i=1}^n -\log P(\mathbf{y}^{(i)} \mid \mathbf{x}^{(i)})
= \sum_{i=1}^n l(\mathbf{y}^{(i)}, \hat{\mathbf{y}}^{(i)}),\\
l(\mathbf{y}, \hat{\mathbf{y}}) = - \sum_{j=1}^q y_j \log \hat{y}_j.
$$

### softmax及其导数

将损失函数化为oj的形状
$$
\begin{split}\begin{aligned}
l(\mathbf{y}, \hat{\mathbf{y}}) &=  - \sum_{j=1}^q y_j \log \frac{\exp(o_j)}{\sum_{k=1}^q \exp(o_k)} \\
&= \sum_{j=1}^q y_j \log \sum_{k=1}^q \exp(o_k) - \sum_{j=1}^q y_j o_j\\
&= \log \sum_{k=1}^q \exp(o_k) - \sum_{j=1}^q y_j o_j.
\end{aligned}\end{split}\\
$$
下面是损失函数关于oj的导数，手写推导如图
$$
\partial_{o_j} l(\mathbf{y}, \hat{\mathbf{y}}) = \frac{\exp(o_j)}{\sum_{k=1}^q \exp(o_k)} - y_j = \mathrm{softmax}(\mathbf{o})_j - y_j.
$$
![img](C:\Users\裴英豪\Desktop\思维导图总结\第二个月\深度学习笔记\source\softmax导数推导.jpg)



## 熵

### 信息论的熵

我们来先看一个公式$H[P] = \sum_j - P(j) \log P(j).$，我们先来看个例子：

> 小张的女朋友发消息有个习惯，不喜欢一句发完，喜欢一个词一个词的蹦，有一天，小张的女朋友发了：我们。这时候小张无法理解下边的内容是什么，就没有放心上，因为这个时候小张女朋友信息量很低，‘我们’这个词后可以跟很多东西，比如我们吃，我们喝，我们玩等等，我们先假设一共有十个可能，那么此刻的信息熵就是-10\*0.1\*log0.1约为2.3。
>
> 之后小张女朋友又发了个‘要’，‘我们要’的后边可能跟的东西假设缩减到了4中，那么此时的信息熵为4\*0.25\*log(0.25)约为1.4，此时看出，我们获得的信息越多，我们的熵越低。
>
> 小张女朋友最后发了‘还是分开吧’，‘我们要不还是分开吧’，以普遍理性而论，小张的爱情结束了，但是此刻我们看一下获得的信息的熵：1\*1\*log（1）=0,小张女朋友的信息熵终于为0了，这个例子希望能帮助你理解什么是信息熵。

### 信息熵与softmax

一个表现很好的softmax呈现的结果应当是准确的落在了真实的分类上，不会把任何可能性分给那些不属于真实样本的类目上，这和信息熵有一定的共同之处。

## 数值稳定性和模型初始化

#### 梯度消失与梯度爆炸

在神经网络中，误差的反向传播迭代公式为:
$$
\delta ^{(l)}=f'_l(z^{(l)}) ⊙ (𝑾^{(𝑙+1)})^T𝛿^{(𝑙+1)}
$$
误差从输出层反向传播时，在每一层都要乘以该层激活函数的导数.当我们使用sigmoid函数:logistic函数或者Tanh函数时，他们的导数图像如：

![1684901450332](C:\Users\裴英豪\Desktop\思维导图总结\第二个月\深度学习笔记\source\sigmoid grad.png)

他们的导数都小于1，在饱和区（正负很大的区域）导数更是接近于0，那么误差在不断地乘的时候就会衰减，如果层数过多甚至会消失，导致最开始基层的作用几乎为0，这种现象称为梯度消失。

同样的，如果我们使用另外的激活函数或者模型上跟mlp有区别，他的梯度在深度较深的情况下会出现梯度爆炸问题，我们用一个很简单的例子来说明:

```python
%matplotlib inline
import torch
from d2l import torch as d2l

M = torch.normal(0, 1, size=(4,4))
print('一个矩阵 \n',M)
for i in range(100):
    M = torch.mm(M,torch.normal(0, 1, size=(4, 4)))

print('乘以100个矩阵后\n', M)

>>一个矩阵 
 tensor([[-0.7476, -0.5905,  0.4594, -0.4446],
        [-0.6868, -0.1769, -1.6432, -1.1101],
        [ 0.2385, -1.6487,  1.3177,  0.1729],
        [ 0.7691, -0.2316, -1.6668, -0.2459]])
乘以100个矩阵后
 tensor([[ 1.2614e+24, -2.9730e+24, -2.1424e+24, -3.2076e+24],
        [ 5.6098e+24, -1.3223e+25, -9.5285e+24, -1.4266e+25],
        [-2.1604e+24,  5.0921e+24,  3.6694e+24,  5.4939e+24],
        [ 2.1933e+24, -5.1700e+24, -3.7257e+24, -5.5779e+24]])
```





#### 这里我们先简单介绍一种参数初始化的方法来解决（减轻）

**Xavier初始化**

我们先假设权重都是从同一分布中独立收取出来的，这个分布的期望为0，他的方差为$\sigma^2$.我们不要求他是高斯分布，我们来假设这个层的全连接输出$o_i$的分布：$o_{i} = \sum_{j=1}^{n_\mathrm{in}} w_{ij} x_j$,不难理解，实际上是权重求和嘛。不过我们假设输入$x_j$的输入也有零均值和方差$\gamma^2$，他们均独立，我们来看看输出$o_j$的平均值和方差:
$$
\begin{split}\begin{aligned}
    E[o_i] & = \sum_{j=1}^{n_\mathrm{in}} E[w_{ij} x_j] \\&= \sum_{j=1}^{n_\mathrm{in}} E[w_{ij}] E[x_j] \\&= 0, \\
    \mathrm{Var}[o_i] & = E[o_i^2] - (E[o_i])^2 \\
        & = \sum_{j=1}^{n_\mathrm{in}} E[w^2_{ij} x^2_j] - 0 \\
        & = \sum_{j=1}^{n_\mathrm{in}} E[w^2_{ij}] E[x^2_j] \\
        & = n_\mathrm{in} \sigma^2 \gamma^2.
\end{aligned}\end{split}
$$
因为$n_\mathrm{in} \sigma^2 $相当于是我们人为给的一个扰动，一个系数，一个决定了前向的不断传递给结果方差大小带来的影响，所以我们应该尽量减少它的影响，所以我们令$n_\mathrm{in} \sigma^2 = 1$。但是我们令$n_\mathrm{in} \sigma^2=1 $只保证了前向传播的时候数值不会有很大的变动，但是反向传播呢？类比思考，那我们应该需要让$n_\mathrm{out} \sigma^2 = 1$，但是两者不可得兼，于是我们进行满足:
$$
\begin{aligned}
\frac{1}{2} (n_\mathrm{in} + n_\mathrm{out}) \sigma^2 = 1 \text{ 或等价于 }
\sigma = \sqrt{\frac{2}{n_\mathrm{in} + n_\mathrm{out}}}.
\end{aligned}\\
$$
值得注意的是，这里的in和out实际上都是参数矩阵的一维，我们用$n_i$表示第i层的输入，用$n_{i+1}$表示第i+1层的输入，那么我们的方差就可以表达成:
$$
\begin{aligned}
\frac{1}{2} (n_\mathrm{i} + n_\mathrm{i+1}) \sigma^2 = 1 \text{ 或等价于 }
\sigma = \sqrt{\frac{2}{n_\mathrm{i} + n_\mathrm{i+1}}}.
\end{aligned}
$$
我们可以使用高斯分布$N(0,\sigma^2)=N(0,\frac{2}{n_\mathrm{in} + n_\mathrm{out}})$，进行初始化，但是也可以改用均匀分布：注意均匀分布$U(−a,a)$的方差为$\frac{a^2}{3}$。 将$\frac{a^2}{3}$代入到$\sigma ^2$的条件中，将得到初始化值域：
$$
U\left(-\sqrt{\frac{6}{n_\mathrm{in} + n_\mathrm{out}}}, \sqrt{\frac{6}{n_\mathrm{in} + n_\mathrm{out}}}\right).
$$

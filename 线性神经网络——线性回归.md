## 从零开始实现线性回归

### 我们的人工数据如何生成



```python
def synthetic_data(w, b, num_examples):  #@save
    """生成y=Xw+b+噪声"""
    X = torch.normal(0, 1, (num_examples, len(w)))#x的值随机生成在（0，1）之间，形状为[样本的长度，参数w的长度]
    y = torch.matmul(X, w) + b#先线性生成y
    y += torch.normal(0, 0.01, y.shape)#给y加上随机的噪声
    return X, y.reshape((-1, 1))#将y转变成列向量，值得注意的是一个方向是-1意味着在reshape的时候不需要限定那个维度的长短，只需要考虑满足其他确定的

true_w = torch.tensor([2, -3.4])#设定真值w
true_b = 4.2#设定真实的b
features, labels = synthetic_data(true_w, true_b, 1000)#生成人工数据集
```

### 使用小批量方法，每次选取一部分的数据，以减少对计算的耗费

```python
def data_iter(batch_size, features, labels):#传入的参数分别是小批量大小，数据的特征，数据的标签
    num_examples = len(features)
    indices = list(range(num_examples))
    # 这些样本是随机读取的，没有特定的顺序
    random.shuffle(indices)#shuffle相当于获取了一个list然后打乱list中元素的顺序
    for i in range(0, num_examples, batch_size):
        batch_indices = torch.tensor(
            indices[i: min(i + batch_size, num_examples)])
        yield features[batch_indices], labels[batch_indices]#每次输出batch大小的
```

> random.shuffle读入一个list，在shuffle之后。list内的元素的排序会被打乱

![1684138915993](C:\Users\裴英豪\Desktop\思维导图总结\第二个月\深度学习笔记\source\random,shuffle.png)

> yield：yield可以简单理解为return操作，但和return又有很大的区别，执行完return，当前函数就终止了，函数内部的所有数据，所占的内存空间，全部都没有了。而yield在返回数据的同时，还保存了当前的执行内容，当你再一次调用这个函数时，他会找到你在此函数中的yield关键字，然后从yield的下一句开始执行。

![1684141300521](C:\Users\裴英豪\Desktop\思维导图总结\第二个月\深度学习笔记\source\yield的使用.png)



### 初始化模型参数

```
w = torch.normal(0, 0.01, size=(2,1), requires_grad=True)
b = torch.zeros(1, requires_grad=True)
```



### 定义模型、损失函数、优化算法

1. ```python
   def linreg(X, w, b):  #@save
       """线性回归模型"""
       return torch.matmul(X, w) + b
   def squared_loss(y_hat, y):  #@save
       """均方损失"""
       return (y_hat - y.reshape(y_hat.shape)) ** 2 / 2
   def sgd(params, lr, batch_size):  #@save
       """小批量随机梯度下降"""
       with torch.no_grad():#和detach有异曲同工之妙，都是为了逃脱autograd的追踪
           for param in params:
               param -= lr * param.grad / batch_size
               param.grad.zero_()
   ```


### 训练

> 要执行以下循环：
>
> ​		初始化参数
>
> ​		重复以下训练，直至完成：
>
> ​					计算梯度g←∂(w,b)1|B|∑i∈Bl(x(i),y(i),w,b)
>
> ​					更新参数

```python
lr=0.001#定义学习率
num_epochs=10#学习轮数
net=linreg#线性回归
loss=squared_loss#loss的损失函数是平方损失函数

for epoch in range(num_epochs):
    for X,y in data_iter(batch_size,features,labels):#只对小批量内部的数据进行梯度计算
        l=loss(net(X,w,b),y)#X和y的小批量损失
        l.sum().backward()#反向梯度传播
        sgd([w,b],lr,batch_size)#使用参数梯度更新
    with torch.no_grad():
        train_l = loss(net(features, w, b), labels)
        print(f'epoch {epoch + 1}, loss {float(train_l.mean()):f}')
        
```





## 简洁的实现线性神经网络

### 读取数据集

```
def load_array(data_arrays, batch_size, is_train=True):  #@save
    """构造一个PyTorch数据迭代器"""
    dataset = data.TensorDataset(*data_arrays)
    return data.DataLoader(dataset, batch_size, shuffle=is_train)

batch_size = 10
data_iter = load_array((features, labels), batch_size)
```

> data.Dataloader是pytorch中读取数据集的函数，并构造python迭代器，并从迭代器中获取每轮不同的数据。



```
next(iter(data_iter))
```

>这里我们使用`iter`构造Python迭代器，并使用`next`从迭代器中获取第一项。（此类迭代器相当于把for的运算给每一次卡在了输出，下次输出时沿着上次的输出继续进行）



### 使用sequential容器存储多层神经网络。

> Sequential一个有序的容器，神经网络模块将按照在传入构造器的顺序依次被添加到计算图中执行， 同时以神经网络模块为元素的有序字典也可以作为传入参数。

```
from torch import nn

net = nn.Sequential(nn.Linear(2, 1))#输入两个参数2，输出一个标量1
```

这次只用了一层线性神经网络。

### 设置初始参数

```
net[0].weight.data.normal_(0, 0.01)
net[0].bias.data.fill_(0)
```



### 定义损失函数

平方$L_2$范数

```
loss = nn.MSELoss()
```

### 定义优化算法

```
trainer = torch.optim.SGD(net.parameters(), lr=0.03)
```

### 训练

```
num_epochs = 3
for epoch in range(num_epochs):
    for X, y in data_iter:
        l = loss(net(X) ,y)
        trainer.zero_grad()
        l.backward()
        trainer.step()
    l = loss(net(features), labels)
    print(f'epoch {epoch + 1}, loss {l:f}')
```

```
<<epoch 1, loss 0.000183
epoch 2, loss 0.000101
epoch 3, loss 0.000101
```


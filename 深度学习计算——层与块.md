> 本篇额外参考了[PyTorch神经网络工具箱](http://www.feiguyunai.com/index.php/2022/09/24/ml-pytorch-base-03/#31)

## 层与块

单个神经网络可以被这么看待，（1）接受一些输入； （2）生成相应的标量输出； （3）具有一组相关 *参数*（parameters），更新这些参数可以优化某目标函数。

**再复杂的神经网络也可以由简单的层和块组成**，总的来说，我们可以说神经网络是很多层构成的网络，加上我们设置的损失函数和优化器（optimizer），如下图所示：

> ![img](C:\Users\裴英豪\Desktop\思维导图总结\第二个月\深度学习笔记\source\神经网络核心组件.png)

我们以前用过sequential来生成一些层，今天我们来进一步总结一下，首先我么看我torch.nn里有哪些主要工具:

> ![img](C:\Users\裴英豪\Desktop\思维导图总结\第二个月\深度学习笔记\source\PyTorch实现神经网络主要工具及相互关系.png)

**Module和functional**里有些函数都是对应的，nn.Module的layer继承自Module类，可以自行提取科学系的参数，而nn.functional更类似纯函数。**在卷积层，全连接层，dropout层等含有可学习的参数，一般使用nn.Module,而激活函数、池化层不含可学习参数，可以使用functional对应的函数**。

我的理解是，nn.Module帮助我们省却了自动梯度（autograd）的一些繁琐，并且nvidia的程序员给底层设计了很好的并发，所以在多层的神经网络或者单个层里，能找到nn.Module里你需要的，就直接调用而不是自己写。

对于nn.functional有个很重要的点**nn.functional.xxx无法与nn.Sequential结合使用**，而且functional每次使用的时候都要手动传入weight和bias，不利于代码复用。

总的来说，它俩功能相同，但PyTorch官方推荐：具有学习参数的（例如，conv2d、 linear、batch_norm、dropout等）情况采用nn.Xxx方式，没有学习参数的（例如，maxpool, loss func, activation func）等情况选择使用nn.functional.xxx或者nn.Xxx方式。3.5节中使用激活层，我们采用无学习参数的F.relu方式来实现，即nn.functional.xxx方式。

#### 构建模型方法

PyTorch构建模型大致有以下3种方式。
1) 继承nn.Module基类构建模型。
2) 使用nn.Sequential按层顺序构建模型。
3) 继承nn.Module基类构建模型，又使用相关模型容器(如nn.Sequential,nn.ModuleList,nn.ModuleDict等）进行封装。

这仨方法第一种最常见，第二种最简单（很适合入门得小模型），第三种比较灵活但是复杂，我们这里三个都展示下，但是我们只对前两个进行分享。（以下图的模型为例）

![img](C:\Users\裴英豪\Desktop\思维导图总结\第二个月\深度学习笔记\source\example.png)

**1) 继承nn.Module基类构建模型。**

利用这种方法构建模型，先定义一个类，使之继承nn.Module基类。把模型中需要用到的层放在构造函数__init__()中，在forward方法中实现模型的正向传播。具体代码如下。

step 1 导入模块

```python
import torch
from torch import nn
import torch.nn.functional as F
```

step 2 构建模型

```python
class Model_Seq(nn.Module):
    """
    通过继承基类nn.Module来构建模型
    """
    def __init__(self, in_dim, n_hidden_1, n_hidden_2, out_dim):
        super(Model_Seq, self).__init__()
        self.flatten = nn.Flatten()#展平层
        self.linear1= nn.Linear(in_dim, n_hidden_1)#线性层
        self.bn1=nn.BatchNorm1d(n_hidden_1)#批规范化，把所有数据给变为统一的分布
        self.linear2= nn.Linear(n_hidden_1, n_hidden_2)#第二个线性层
        self.bn2 = nn.BatchNorm1d(n_hidden_2)#第二个批规范层
        self.out = nn.Linear(n_hidden_2, out_dim)#输出层
        
 
    def forward(self, x):#定义前向传播
        x=self.flatten(x)
        x=self.linear1(x)
        x=self.bn1(x)
        x = F.relu(x)
        x=self.linear2(x)
        x=self.bn2(x)
        x = F.relu(x)
        x=self.out(x)
        x = F.softmax(x,dim=1)
        return x
```

step 3 构建超参数并查看模型

```python
##对一些超参数赋值
in_dim, n_hidden_1, n_hidden_2, out_dim=28 * 28, 300, 100, 10
model_seq= Model_Seq(in_dim, n_hidden_1, n_hidden_2, out_dim)
print(model_seq)
```

输出结果如下

```python
Model_Seq(
  (flatten): Flatten(start_dim=1, end_dim=-1)
  (linear1): Linear(in_features=784, out_features=300, bias=True)
  (bn1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (linear2): Linear(in_features=300, out_features=100, bias=True)
  (bn2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (out): Linear(in_features=100, out_features=10, bias=True)
)
```



**2) 使用nn.Sequential按层顺序构建模型。**

使用nn.Sequential构建模型，因其内部实现了forward函数，因此可以不用写forward函数。nn.Sequential里面的模块按照先后顺序进行排列的，所以必须确保前一个模块的输出大小和下一个模块的输入大小是一致的。使用这种方法一般构建较简单的模型。 以下是使用nn.Sequential搭建模型的几种等价方法。

1.利用可变参数

python可以接受不定长的参数，pytorch亦如此，如nn.Sequential(*args)就是：

1. 导入模块

   ```python
   import torch
   from torch import nn
   ```

   

2. 构建模型

   这也是我们前边经常使用的一种方法

   ```python
   Seq_arg = nn.Sequential(
       nn.Flatten(),
       nn.Linear(in_dim,n_hidden_1),
       nn.BatchNorm1d(n_hidden_1),
       nn.ReLU(),
       nn.Linear(n_hidden_1, n_hidden_2),
       nn.BatchNorm1d(n_hidden_2),
       nn.ReLU(),         
       nn.Linear(n_hidden_2, out_dim),
       nn.Softmax(dim=1)
   )
   ```

   

3. 查看模型

   ```python
   in_dim, n_hidden_1, n_hidden_2, out_dim=28 * 28, 300, 100, 10
   print(Seq_arg)
   ```

   输出结果如下

   ```python
   Sequential(
     (0): Flatten(start_dim=1, end_dim=-1)
     (1): Linear(in_features=784, out_features=300, bias=True)
     (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
     (3): ReLU()
     (4): Linear(in_features=300, out_features=100, bias=True)
     (5): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
     (6): ReLU()
     (7): Linear(in_features=100, out_features=10, bias=True)
     (8): Softmax(dim=1)
   )
   ```

   

2.使用add_module方法

上边的Sequential方法有个缺点就是不能够给每个层指定名称，如果我们想要给层赋名字，就可以使用add_module或OrdereDict方法。

1. 构建模型

   ```python
   Seq_module = nn.Sequential()#先生成一个空的Sequential容器
   Seq_module.add_module("flatten",nn.Flatten())
   Seq_module.add_module("linear1",nn.Linear(in_dim,n_hidden_1))
   Seq_module.add_module("bn1",nn.BatchNorm1d(n_hidden_1))
   Seq_module.add_module("relu1",nn.ReLU())
   Seq_module.add_module("linear2",nn.Linear(n_hidden_1, n_hidden_2))
   Seq_module.add_module("bn2",nn.BatchNorm1d(n_hidden_2))
   Seq_module.add_module("relu2",nn.ReLU())         
   Seq_module.add_module("out",nn.Linear(n_hidden_2, out_dim))
   Seq_module.add_module("softmax",nn.Softmax(dim=1))
   ```

   

2. 查看模型

```python
in_dim, n_hidden_1, n_hidden_2, out_dim=28 * 28, 300, 100, 10
print(Seq_module)
```

运行结果如下

```python
Sequential(
    (flatten): Flatten(start_dim=1, end_dim=-1)
    (linear1): Linear(in_features=784, out_features=300, bias=True)
    (bn1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu1): ReLU()
    (linear2): Linear(in_features=300, out_features=100, bias=True)
    (bn2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu2): ReLU()
    (out): Linear(in_features=100, out_features=10, bias=True)
    (softmax): Softmax(dim=1)
)
```

这样我们在进行模型管理的时候就会更加简单了。



**3) 继承nn.Module基类构建模型并使用模型容器来构建模型**

我们这里介绍使用nn.ModuleDict模型容器，他向我们展示了如何增强模型的可读性，并减少了代码量，对部分结构进行了封装

1. 导入模块

   ```python
   import torch
   from torch import nn
   ```

   

2. 构建模型

   ```python
   class Model_dict(nn.Module):
       
       def __init__(self,in_dim, n_hidden_1,n_hidden_2,out_dim):
           super(Model_dict, self).__init__()
           self.layers_dict = nn.ModuleDict({"flatten":nn.Flatten(),
           "linear1":nn.Linear(in_dim,n_hidden_1),
           "bn1":nn.BatchNorm1d(n_hidden_1),
           "relu":nn.ReLU(),
           "linear2":nn.Linear(n_hidden_1, n_hidden_2),
           "bn2":nn.BatchNorm1d(n_hidden_2),
           "out":nn.Linear(n_hidden_2, out_dim),
           "softmax":nn.Softmax(dim=1)
           })
       def forward(self,x):
           layers = ["flatten","linear1","bn1","relu","linear2","bn2","relu","out","softmax"]
           for layer in layers:
               x = self.layers_dict[layer](x)
           return x
   ```

   

3. 查看模型

   ```python
   in_dim, n_hidden_1, n_hidden_2, out_dim=28 * 28, 300, 100, 10
   model_dict = Model_dict(in_dim, n_hidden_1, n_hidden_2, out_dim)
   print(model_dict)
   ```

   运行结果如下

   ```python
   Model_dict(
     (layers_dict): ModuleDict(
       (flatten): Flatten(start_dim=1, end_dim=-1)
       (linear1): Linear(in_features=784, out_features=300, bias=True)
       (bn1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
       (relu): ReLU()
       (linear2): Linear(in_features=300, out_features=100, bias=True)
       (bn2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
       (out): Linear(in_features=100, out_features=10, bias=True)
       (softmax): Softmax(dim=1)
     )
   )
   ```

   



上边我们讨论了层，但是真正的神经网络由很多层组成，他们中间的一部分是重复的，就像是汉堡=面包+肉+芝士+肉+芝士+肉+芝士......，为了实现复杂的网络结构的简化，我们引入块（block），以描述单个层、多个层构成的组件，就如下图所示。

![../_images/blocks.svg](C:\Users\裴英豪\Desktop\思维导图总结\第二个月\深度学习笔记\source\blocks.svg)

残差网络就是其中典范，以ResNet18为例，网络的基本架构是ResNet，网络的深度是18层。但是这里的网络深度指的是网络的权重层，也就是包括池化，激活，线性层。而不包括批量化归一层，池化层。

ResNet的残差快如下图所示:

![img](C:\Users\裴英豪\Desktop\思维导图总结\第二个月\深度学习笔记\source\ResNet的块.png)

残差块有两种，一种是正常的模块方式，如左图，将输入与输出相加，然后应用激活函数ReLU。 另一种是为使输入与输出形状一致，需添加通过1×1卷积调整通道和分辨率，如右图所示。这些模块中用到卷积层、批量规范化层，我们后边会学到，这里我们只需要了解这些是网络层即可。

1. 定义图左的残差模块

   ```python
   import torch
   import torch.nn as nn
   from torch.nn import functional as F
   
   
   class RestNetBasicBlock(nn.Module):
       def __init__(self,in_channels,out_channels,stride):
           super(RestNetBasicBlock,self).__init__()
           self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1)
           self.bn1 = nn.BatchNorm2d(out_channels)
           self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=stride, padding=1)
           self.bn2 = nn.BatchNorm2d(out_channels)
           pass
       
       def forward(self,x):
           output = self.conv1(x)
           output = F.relu(self.bn1(output))
           output = self.conv2(output)
           output = self.bn2(output)
           return F.relu(x + output)
   ```

   

2. 定义图右的残差模块

   ```python
   class RestNetDownBlock(nn.Module):
       def __init__(self, in_channels, out_channels, stride):
           super(RestNetDownBlock, self).__init__()
           self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride[0], padding=1)
           self.bn1 = nn.BatchNorm2d(out_channels)
           self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=stride[1], padding=1)
           self.bn2 = nn.BatchNorm2d(out_channels)
           self.extra = nn.Sequential(
               nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride[0], padding=0),
               nn.BatchNorm2d(out_channels)
           )
   
       def forward(self, x):
           extra_x = self.extra(x)
           output = self.conv1(x)
           out = F.relu(self.bn1(output))
   
           out = self.conv2(out)
           out = self.bn2(out)
           return F.relu(extra_x + out)
   ```

3. 组合两个模块得到现代经典ResNet18网络结构

   ```python
   class RestNet18(nn.Module):
       def __init__(self):
           super(RestNet18, self).__init__()
           self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3)
           self.bn1 = nn.BatchNorm2d(64)
           self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)
   
           self.layer1 = nn.Sequential(RestNetBasicBlock(64, 64, 1),
                                       RestNetBasicBlock(64, 64, 1))
   
           self.layer2 = nn.Sequential(RestNetDownBlock(64, 128, [2, 1]),
                                       RestNetBasicBlock(128, 128, 1))
   
           self.layer3 = nn.Sequential(RestNetDownBlock(128, 256, [2, 1]),
                                       RestNetBasicBlock(256, 256, 1))
   
           self.layer4 = nn.Sequential(RestNetDownBlock(256, 512, [2, 1]),
                                       RestNetBasicBlock(512, 512, 1))
   
           self.avgpool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
   
           self.fc = nn.Linear(512, 10)
   
       def forward(self, x):
           out = self.conv1(x)
           out = self.layer1(out)
           out = self.layer2(out)
           out = self.layer3(out)
           out = self.layer4(out)
           out = self.avgpool(out)
           out = out.reshape(x.shape[0], -1)
           out = self.fc(out)
           return out
   ```

   